{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instaloader\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded session from session-faizgram.\n"
     ]
    }
   ],
   "source": [
    "L = instaloader.Instaloader()\n",
    "\n",
    "USERNAME = \"faizgram\"\n",
    "SESSION_FILE = \"session-faizgram\"\n",
    "\n",
    "try:\n",
    "    L.load_session_from_file(username=USERNAME, filename=SESSION_FILE)\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Session file not found. Please log in manually using Instaloader CLI.\")\n",
    "    exit()\n",
    "except instaloader.ConnectionException as e:\n",
    "   print(f\"Connection exception {e}\")\n",
    "   exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def by_hashtag(hashtag, limit, profile_name=None, location=None):\n",
    "    post_count = 0\n",
    "    hashtag_posts = pd.DataFrame(columns=[\"Thumb\", \"Caption\", \"Comment\"])\n",
    "    thumbs = []\n",
    "    captions = []\n",
    "    comments = []\n",
    "    dates = []\n",
    "\n",
    "    posts = L.get_hashtag_posts(hashtag)\n",
    "\n",
    "    for post in posts:\n",
    "        if post.caption:\n",
    "            thumbs.append(post.url)\n",
    "            captions.append(post.caption)\n",
    "            comments.append(post.comments)\n",
    "            dates.append(post.date)\n",
    "            # print(f\"Extracted caption: {post.caption}\")\n",
    "        \n",
    "        post_count += 1\n",
    "        if post_count >= limit:\n",
    "            break\n",
    "\n",
    "    hashtag_posts[\"Thumb\"] = thumbs\n",
    "    hashtag_posts[\"Caption\"] = captions\n",
    "    hashtag_posts[\"Date\"] = dates\n",
    "    hashtag_posts[\"Comment\"] = comments\n",
    "\n",
    "    return hashtag_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = by_hashtag(\"unikl\", 5, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thumb</th>\n",
       "      <th>Caption</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://instagram.fkul4-3.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>‚ù§Ô∏è‚ù§Ô∏èRUN‚ù§Ô∏è‚ù§Ô∏è\\nSouthern Bundle Collection\\n\\nNB ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-10 13:39:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://instagram.fkul4-5.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>‚ù§Ô∏è‚ù§Ô∏èCOURT‚ù§Ô∏è‚ù§Ô∏è\\nSouthern Bundle Collection\\n\\nA...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-10 12:35:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://instagram.fkul4-4.fna.fbcdn.net/v/t39....</td>\n",
       "      <td>‚ù§Ô∏è‚ù§Ô∏èCOURT‚ù§Ô∏è‚ù§Ô∏è\\nSouthern Bundle Collection\\n\\nM...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-10 11:50:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://instagram.fkul4-5.fna.fbcdn.net/v/t39....</td>\n",
       "      <td>Betul ke apa yang kita fikir akan jadi doa? \\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-10 11:39:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://instagram.fkul4-4.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>Posted @withregram ‚Ä¢ @cmt.uniklmiit üåü Congratu...</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-12-10 11:32:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Thumb  \\\n",
       "0  https://instagram.fkul4-3.fna.fbcdn.net/v/t51....   \n",
       "1  https://instagram.fkul4-5.fna.fbcdn.net/v/t51....   \n",
       "2  https://instagram.fkul4-4.fna.fbcdn.net/v/t39....   \n",
       "3  https://instagram.fkul4-5.fna.fbcdn.net/v/t39....   \n",
       "4  https://instagram.fkul4-4.fna.fbcdn.net/v/t51....   \n",
       "\n",
       "                                             Caption  Comment  \\\n",
       "0  ‚ù§Ô∏è‚ù§Ô∏èRUN‚ù§Ô∏è‚ù§Ô∏è\\nSouthern Bundle Collection\\n\\nNB ...        0   \n",
       "1  ‚ù§Ô∏è‚ù§Ô∏èCOURT‚ù§Ô∏è‚ù§Ô∏è\\nSouthern Bundle Collection\\n\\nA...        0   \n",
       "2  ‚ù§Ô∏è‚ù§Ô∏èCOURT‚ù§Ô∏è‚ù§Ô∏è\\nSouthern Bundle Collection\\n\\nM...        0   \n",
       "3  Betul ke apa yang kita fikir akan jadi doa? \\n...        0   \n",
       "4  Posted @withregram ‚Ä¢ @cmt.uniklmiit üåü Congratu...        1   \n",
       "\n",
       "                 Date  \n",
       "0 2024-12-10 13:39:19  \n",
       "1 2024-12-10 12:35:53  \n",
       "2 2024-12-10 11:50:29  \n",
       "3 2024-12-10 11:39:19  \n",
       "4 2024-12-10 11:32:21  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mohdf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mohdf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mohdf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "bs4 is not installed, `malaya.text.function.remove_html_tags` will use regex\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mohdf\\anaconda\\envs\\work\\Lib\\site-packages\\malaya\\__init__.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://f000.backblazeb2.com/file/malaya-model/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     17\u001b[0m __home__, _ \u001b[38;5;241m=\u001b[39m get_home(package\u001b[38;5;241m=\u001b[39mpackage, package_version\u001b[38;5;241m=\u001b[39mversion)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m augmentation\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dictionary\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generator\n",
      "File \u001b[1;32mc:\\Users\\mohdf\\anaconda\\envs\\work\\Lib\\site-packages\\malaya\\augmentation\\__init__.py:10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Malaya Natural Language Toolkit\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2019 Malaya Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# URL: <https://malaya.readthedocs.io/>\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# For license information, see https://github.com/huseinzol05/Malaya/blob/master/LICENSE\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m abstractive\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rules\n",
      "File \u001b[1;32mc:\\Users\\mohdf\\anaconda\\envs\\work\\Lib\\site-packages\\malaya\\augmentation\\abstractive.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupervised\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Translation\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[0;32m      5\u001b[0m available_huggingface \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmesolitica/translation-t5-tiny-standard-bahasa-cased\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSize (MB)\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m139\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     },\n\u001b[0;32m     66\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\mohdf\\anaconda\\envs\\work\\Lib\\site-packages\\malaya\\torch_model\\huggingface.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse_dependency\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DependencyGraph\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrouge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m postprocess_summary, find_kata_encik\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Base\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     37\u001b[0m     T5ForSequenceClassification,\n\u001b[0;32m     38\u001b[0m     T5ForTokenClassification,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     T5Embedding,\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaModelEmbedding\n",
      "File \u001b[1;32mc:\\Users\\mohdf\\anaconda\\envs\\work\\Lib\\site-packages\\malaya\\torch_model\\base.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBase\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import pipeline\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Tesseract path (Windows 10/11)\n",
    "tesseract_path = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Unix/Linux\n",
    "\n",
    "# MacOS\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text: \n",
      "Extracted text: \n",
      "Extracted text: \n",
      "Extracted text: \n",
      "Extracted text: xX XX KX X XK XK X\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img_texts = []\n",
    "for img_url in posts[\"Thumb\"]:\n",
    "    response = requests.get(img_url)\n",
    "    img_text = pytesseract.image_to_string(Image.open(BytesIO(response.content)))\n",
    "    print(f\"Extracted text: {img_text}\")\n",
    "    img_texts.append(img_text)\n",
    "\n",
    "posts[\"Thumb Text\"] = img_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thumb</th>\n",
       "      <th>Caption</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Date</th>\n",
       "      <th>Thumb Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://instagram.fkul4-3.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>‚ù§Ô∏è‚ù§Ô∏èRUN‚ù§Ô∏è‚ù§Ô∏è\\nSouthern Bundle Collection\\n\\nNB ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-10 13:39:19</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://instagram.fkul4-5.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>‚ù§Ô∏è‚ù§Ô∏èCOURT‚ù§Ô∏è‚ù§Ô∏è\\nSouthern Bundle Collection\\n\\nA...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-10 12:35:53</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://instagram.fkul4-4.fna.fbcdn.net/v/t39....</td>\n",
       "      <td>‚ù§Ô∏è‚ù§Ô∏èCOURT‚ù§Ô∏è‚ù§Ô∏è\\nSouthern Bundle Collection\\n\\nM...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-10 11:50:29</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://instagram.fkul4-5.fna.fbcdn.net/v/t39....</td>\n",
       "      <td>Betul ke apa yang kita fikir akan jadi doa? \\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-10 11:39:19</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://instagram.fkul4-4.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>Posted @withregram ‚Ä¢ @cmt.uniklmiit üåü Congratu...</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-12-10 11:32:21</td>\n",
       "      <td>xX XX KX X XK XK X\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Thumb  \\\n",
       "0  https://instagram.fkul4-3.fna.fbcdn.net/v/t51....   \n",
       "1  https://instagram.fkul4-5.fna.fbcdn.net/v/t51....   \n",
       "2  https://instagram.fkul4-4.fna.fbcdn.net/v/t39....   \n",
       "3  https://instagram.fkul4-5.fna.fbcdn.net/v/t39....   \n",
       "4  https://instagram.fkul4-4.fna.fbcdn.net/v/t51....   \n",
       "\n",
       "                                             Caption  Comment  \\\n",
       "0  ‚ù§Ô∏è‚ù§Ô∏èRUN‚ù§Ô∏è‚ù§Ô∏è\\nSouthern Bundle Collection\\n\\nNB ...        0   \n",
       "1  ‚ù§Ô∏è‚ù§Ô∏èCOURT‚ù§Ô∏è‚ù§Ô∏è\\nSouthern Bundle Collection\\n\\nA...        0   \n",
       "2  ‚ù§Ô∏è‚ù§Ô∏èCOURT‚ù§Ô∏è‚ù§Ô∏è\\nSouthern Bundle Collection\\n\\nM...        0   \n",
       "3  Betul ke apa yang kita fikir akan jadi doa? \\n...        0   \n",
       "4  Posted @withregram ‚Ä¢ @cmt.uniklmiit üåü Congratu...        1   \n",
       "\n",
       "                 Date              Thumb Text  \n",
       "0 2024-12-10 13:39:19                          \n",
       "1 2024-12-10 12:35:53                          \n",
       "2 2024-12-10 11:50:29                          \n",
       "3 2024-12-10 11:39:19                          \n",
       "4 2024-12-10 11:32:21  xX XX KX X XK XK X\\n\\n  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mohdf\\anaconda\\envs\\work\\Lib\\site-packages\\malaya\\__init__.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://f000.backblazeb2.com/file/malaya-model/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     17\u001b[0m __home__, _ \u001b[38;5;241m=\u001b[39m get_home(package\u001b[38;5;241m=\u001b[39mpackage, package_version\u001b[38;5;241m=\u001b[39mversion)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m augmentation\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dictionary\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generator\n",
      "File \u001b[1;32mc:\\Users\\mohdf\\anaconda\\envs\\work\\Lib\\site-packages\\malaya\\augmentation\\__init__.py:10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Malaya Natural Language Toolkit\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2019 Malaya Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# URL: <https://malaya.readthedocs.io/>\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# For license information, see https://github.com/huseinzol05/Malaya/blob/master/LICENSE\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m abstractive\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rules\n",
      "File \u001b[1;32mc:\\Users\\mohdf\\anaconda\\envs\\work\\Lib\\site-packages\\malaya\\augmentation\\abstractive.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupervised\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Translation\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[0;32m      5\u001b[0m available_huggingface \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmesolitica/translation-t5-tiny-standard-bahasa-cased\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSize (MB)\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m139\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     },\n\u001b[0;32m     66\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\mohdf\\anaconda\\envs\\work\\Lib\\site-packages\\malaya\\torch_model\\huggingface.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse_dependency\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DependencyGraph\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrouge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m postprocess_summary, find_kata_encik\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Base\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     37\u001b[0m     T5ForSequenceClassification,\n\u001b[0;32m     38\u001b[0m     T5ForTokenClassification,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     T5Embedding,\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmalaya\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaModelEmbedding\n",
      "File \u001b[1;32mc:\\Users\\mohdf\\anaconda\\envs\\work\\Lib\\site-packages\\malaya\\torch_model\\base.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBase\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import malaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # this uses NLTK\n",
    "    # tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # this uses malaya toolkit\n",
    "    tokens = malaya.tokenizer.Tokenizer(text.lower())\n",
    "\n",
    "    # remove alphanumeric word(s)\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum()]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text_data(image_texts, description_texts):\n",
    "    all_texts = image_texts + description_texts\n",
    "    preprocessed_texts = [preprocess_text(text) for text in all_texts]\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'malaya' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m posts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCaption\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m----> 3\u001b[0m    texts\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[32], line 3\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# tokens = word_tokenize(text.lower())\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmalaya\u001b[49m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mSentenceTokenizer(text\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m      4\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalnum()]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_tokens)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'malaya' is not defined"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for text in posts[\"Caption\"]:\n",
    "   texts.append(preprocess_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['southern bundle collection nb 411 all so size kondisi rm 128 free pos semenanjung sbc9uk souternbundle2 asicsbundle mizunobundle b runningshoes unikl uitm upm utm hokabundle tldm tudm pdrm ukm politeknik ipts ansara kasutbundle jlj polis ramd wataniah upsi ipg mitec uia ukm ipta',\n",
       " 'southern bundle collection as1cs bladelyte size 27cm kondisi rm 110 free pos semenanjung sbc8uk souternbundle2 asicsbundle mizunobundle b runningshoes unikl uitm upm utm hokabundle tldm tudm pdrm ukm politeknik ipts ansara kasutbundle jlj polis ramd wataniah upsi ipg mitec uia ukm ipta',\n",
       " 'southern bundle collection m1zuno wave lightning z5 size kondisi rm 178 free pos semenanjung sbc7uk souternbundle2 asicsbundle mizunobundle b runningshoes unikl uitm upm utm hokabundle tldm tudm pdrm ukm politeknik ipts ansara kasutbundle jlj polis ramd wataniah upsi ipg mitec uia ukm ipta',\n",
       " 'betul ke apa yang kita fikir akan jadi doa otak kita ni allah cipta untuk kita gunakan sebaiknya bila kita fikir sesuatu soalan otak kita akan force semua sel dalam tubuh badan kita untuk cari jawapan tersebut semua perkara ni bukan syirik tapi semua ni terbukti dalam sains otak kita mula berhubung bila mana kita mula masukkan input baru dan ada huraian tentang apa yang kita fikirkan adalah doa dari abu hurairah ra rasulullah saw bersabda allah swt berfirman aku allah sesuai dengan persangka hambaku jika dia bersangka baik maka baginya kebaikkan jika dia bersangka buruk maka baginya keburukkan hadis riwayat bukhari muslim kita ni selalunya boleh ke ni boleh ke berjaya boleh ke jadi pandai boleh ke jadi kaya boleh ke sihat boleh ke tenang kita suka bersangka buruk terlebih dahulu sedangkan yang ciptakan kita tu adalah allah allah ni maha kaya maha pemberi rezeki jika kita fikirkan kejayaan maka kejayaanlah kita akan dapat bila kita fikir perkara itu senang maka kesenanganlah kita akan dapat baca balik apa amani share sebelum ni ada 2 takdir dan ada takdir yang kita boleh ubah dengan berdoa iaitu takdir mu alaq jadi mulai dari sekarang ubahlah cara kita fikir ubahlah cara kita berdoa contoh insyaallah saya pasti boleh insyaallah saya berjaya insyaallah saya bahagia insyaallah saya sihat insyaallah saya kuat always have a positive mindset jika kau fikirkan kau boleh kau hampiri kejayaan lots of love amani basha lead you to success inspiration ubahhidup motivation husnuzon bersangkabaik positif positive tenang bahagia kejayaan iamgenius brilliant unikl happy sharing yang baik datang dari allah swt jika ada kurang dalam penjelasan penulisan ianya datang dari diri amani sendiri haza rahmatun min rabbi anda hebat sila share jika rasa bermanfaat',\n",
       " 'posted withregram congratulations to muhammad azeem bin ahmad we are so proud of muhammad azeem a talented final year student in interactive multimedia design for achieving certifications in professional visual design and mastering tools like adobe photoshop and adobe illustrator azeem s passion for design and dedication to honing his skills shine brightly through this achievement your hard work is inspiring and we know this is just the beginning of an exciting journey ahead in the creative industry keep pushing boundaries azeem and continue making an impact with your creativity and vision the future is yours to shape be part of our global community visit for programmes of choice unikl weareunikl uniklmiit uniklproud uniklmilikmara mypremierdigitaltech mypdti creativetalent adobecertified cmtmiit creativemultimediatechnology']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
